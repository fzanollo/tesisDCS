% Performance evaluation
% Benchmark de Dani
% Robar algo de la tesis de Dani para comentar sobre el benchmark?
% Comparación con la versión previa
% Gráficos dcs vs dcs 2, dcs2 vs las otras herramientas

Para realizar las pruebas de performance decidimos usar el mismo conjunto de problemas que el utilizado para examinar el algoritmo original de \DCS en \cite{tesisDani}. En esta sección presentamos los resultados de la comparación versus dicha versión y, además, contra diversos programas del estado del arte. Todos los casos de estudios fueron escritos con la posibilidad de ser escalados en el número de componentes y estados.
\begin{description}
    \item [Transfer Line] Automatización de una fábrica, un dominio de mucho interés en el área de supervisory control. TL consiste de n máquinas conectadas por $n$ buffers cada uno con capacidad de $k$ unidades, termina en una máquina adicional llamada Test Unit.
    
    \item [Dinning Philosophers] Problema clásico de concurrencia. En DP hay $n$ filósofos sentados en una mesa redonda, cada uno comparte un tenedor con sus vecinos aledaños. El objetivo del sistema es controlar el acceso a los tenedores de manera que los filósofos puedan alternar entre comer y pensar; evitando \textit{deadlock} y \textit{starvation}. Adicionalmente, cada filósofo, luego de tomar un tenedor, debe cumplir con $k$ pasos de etiqueta antes de comer.

    \item [Cat and Mouse] Juego de dos jugadores donde cada uno toma turnos para moverse a una casilla adyacente dentro de un mapa de la forma de un corredor dividido en $2k + 1$ áreas. En CM $n$ gatos y la misma cantidad de ratones son colocados en extremos opuestos del corredor. El objetivo es mover a los ratones de manera que no terminen en el mismo lugar que un gato. Los movimientos de los gatos no son controlables. En el centro del corredor hay un agujero que lleva a los ratones a un área segura.
    
    \item [Bidding Workflow] Modela el proceso de evaluación de proyectos de una empresa. El proyecto debe ser aprobado por $n$ equipos. El objetivo es sintetizar un flujo de trabajo que intente llegar a un consenso, es decir, aprobar/rechazar el proyecto cuando todos los equipos lo aceptan/rechazan. La propuesta puede ser reasignada para re-evaluación por un equipo hasta $k$ veces, no se puede reasignar si el equipo ya lo había aceptado. Cuando un equipo lo rechaza $k$ veces el proyecto puede ser rechazado sin consenso. Es un caso de estudio típico del dominio de Business Process Management.
    
    \item [Air-Traffic Management] Representa la torre de control de un aeropuerto, que recibe $n$ peticiones de aterrizaje simultáneas. La torre necesita avisar si tiene permiso para aterrizar o, en caso contrario, en cuál de los $k$ espacios aéreos debe realizar maniobras de espera. El objetivo es que todos los aviones puedan aterrizar de manera segura. El problema solo tiene solución si la cantidad de aviones es menor a la de espacios aéreos ($n<k$).
    
    \item [Travel Agency] Modela una página on-line de ventas de paquetes de viajes. El sistema depende de $n$ servicios de terceros para realizar las reservas (ej. alquiler de auto, compra de pasajes, etc). Los protocolos para utilizar los servicios pueden variar de manera no controlable; una variante es la selección de hasta $k$ atributos (ej. destino del vuelo, clase y fechas). El objetivo del sistema es orquestar los servicios de manera de obtener un paquete de vacaciones completo de ser posible, evitando pagar por paquetes incompletos.
\end{description}

\section{Comparación entre resultados de SCD}
Como ya dijimos, el foco del trabajo no estuvo solo en corregir los errores encontrados en el algoritmo sino también en brindar una mayor seguridad sobre la corrección y completitud de la exploración on the fly. Esto debía hacerse sin perder la buena performance que aportaba la técnica, permitiendo aplicarla a casos de mayor tamaño. Aclaramos que el benchmark se corrió en un equipo de las mismas características para ambas versiones de \DCS.

La comparación se realizó para las dos heurísticas desarrolladas en \cite{tesisDani}, Monotonic Abstraction y Ready Abstraction. Agregamos además una de las heurísticas naif, Breadth First Search, que desarrollamos para depurar el código; esto es en función de mostrar la posible ganancia obtenida a través de una buena heurística.

\begin{figure}[th]
    \centering
    \hspace*{-20mm}
    \begin{subfigure}{0.7\textwidth}
        \includegraphics[width=\linewidth]{figures/benchmark/dcs_vs.pdf}\label{fig:dcs:results:detailed}
        \caption{resultados de comparación \DCS en detalle.}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=0.9\linewidth]{figures/benchmark/dcs_instances.pdf}\label{fig:dcs:results:instances}
        \includegraphics[width=\linewidth]{figures/benchmark/dcs_time.pdf}\label{fig:dcs:results:time}
        \caption{Instancias resueltas totales (arriba) y tiempo de ejecución (abajo) con \DCS.}
    \end{subfigure}
\caption{Resultados de comparación entre \DCS.}
\label{fig:dcs:results}
\end{figure}

En la figura~\ref{fig:dcs:results} se pueden observar los resultados con respecto al algoritmo de \DCS en \cite{tesisDani}. Antes que nada podemos concluir que hay una ganancia al utilizar mejores heurísticas, ya que BFS pierde contra RA tanto en cantidad de instancias como tiempo. Sorprendentemente a pesar de ser muy naif termina compitiendo contra MA. Esto tiene su explicación en el cambio de objetivo, ya que MA fue presentada en \cite{Ciolek:2016:DCS} para \textit{reachability}, en lugar de \textit{non-blocking}.

Nuestra versión (SCD2) mantiene la performance en la mayoría de los casos de estudio. En TA hay algunas instancias nuevas que dan timeout, pero el resto concluyen rápidamente. El único caso donde pierde notoriamente es BW, pero mirando otras herramientas (con la excepción de \MYND) vemos que en general tienen problemas con los casos más grandes de BW. Nuestra suposición es que la implementación \DCS anterior clasificaba estados como $\Goals$ de manera anticipada y posiblemente errónea, quizás casos como los vistos en la sección \ref{chpt:testing}. Esto puede hacer que se llegue a una conclusión apurada que posiblemente devuelva un controlador incorrecto, pero que en el benchmark parezca resolver más instancias.

\section{Comparación con otros programas}

En la comparación entre distintas herramientas es importante destacar que se hacen de forma ilustrativa para justificar que este trabajo presenta una herramienta del estado del arte. No debe usarse esta comparación para justificar que una herramienta es mejor que otra. En especial, cabe recordar que las herramientas no sintetizan soluciones con las mismas características. Supremica garantiza que su controlador es maximalmente permisivo, lo cual puede incurrir en un mayor costo computacional. 

Adicionalmente, al comparar MTSA contra otras herramientas el benchmark es traducido a la sintaxis aceptada por MBP~\cite{Gromyko:2006:TCS}, MYND~\cite{Mattmuller:2010:MYND} y SUPREMICA~\cite{Mohajerani:2014:Supremica}. La traducción se realiza de forma automática y fue probada correcta en \cite{Ciolek:2016:DCS}, sin embargo, la construcción automática de especificaciones puede omitir ciertas optimizaciones de modelado conocidas por los expertos del área. En \cite{Ciolek:2016:DCS} se muestra que las traducciones no implican un aumento exponencial de estados, pero eso no descarta que la traducción pueda afectar los desempeños de las otras herramientas.

Si bien el algoritmo presentado parece perder un poco de escalabilidad en ciertos casos, en la figura \ref{fig:tools:results} podemos ver que sigue estando en una posición competitiva con respecto a otras herramientas del estado del arte. Para que sea más simple la comparación decidimos mostrar solo RA, ya que es la heurística de mejor desempeño, y ambas versiones de \DCS. 

Aunque la nueva versión esté un poco más lejos de \MYND, en calidad de cantidad de intancias y tiempo supera ampliamente el resto de las opciones, referirse a fig \ref{fig:tools:results:time}.
\begin{figure}[th]
    \centering
    \hspace*{-20mm}
    \begin{subfigure}{0.7\textwidth}
        \includegraphics[width=\linewidth]{figures/benchmark/tools_vs.pdf}\label{fig:tools:results:detailed}
        \caption{detalle de comparación entre herramientas.}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=0.9\linewidth]{figures/benchmark/tools_instances.pdf}
        \includegraphics[width=\linewidth]{figures/benchmark/tools_time.pdf}
        \caption{Instancias resueltas totales (arriba) y tiempo de ejecución (abajo) para cada herramienta.}
        \label{fig:tools:results:time}
    \end{subfigure}
\caption{Resultados de comparación entre herramientas.}
\label{fig:tools:results}
\end{figure}
